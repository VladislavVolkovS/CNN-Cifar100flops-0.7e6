{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VladislavVolkovS/CNN-Cifar100flops-0.7e6/blob/main/%D0%A1%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8_cifar100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz8MvHzVyN7A"
      },
      "source": [
        "В данном задании необходимо разработать архитектуру сверточной ИНС, обеспечивающую наибольшую точность при ограничении на количество операций (FLOPs <= 0.707e6). Необходимая точность (accuracy) сети на датасете CIFAR100 - 30%. Желаемая точность (accuracy) сети на датасете CIFAR100 - 45%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMYyjqmdwhME",
        "outputId": "bd0d20de-4a0c-44a6-d509-ad2d402131ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-flops\n",
            "  Downloading keras_flops-0.1.2-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: tensorflow<3.0,>=2.2 in /usr/local/lib/python3.8/dist-packages (from keras-flops) (2.9.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.14.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.29.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (21.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (4.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.51.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (15.0.6.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.1.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.9.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (0.6.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow<3.0,>=2.2->keras-flops) (3.0.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<3.0,>=2.2->keras-flops) (3.2.2)\n",
            "Installing collected packages: keras-flops\n",
            "Successfully installed keras-flops-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-flops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UrGSApyynBU"
      },
      "source": [
        "Импортируем библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hHDmED6typuU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras_flops import get_flops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BrThpNdzoaY"
      },
      "source": [
        "Глобальные константы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yxvhCQZLzr0Z"
      },
      "outputs": [],
      "source": [
        "CLASSES       = 100\n",
        "BATCH_SIZE    = 128\n",
        "LEARNING_RATE = 1e-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKiWO-U8z5bq"
      },
      "source": [
        "Загружаем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-uazE9Dz70f",
        "outputId": "a0d23914-bd8c-4dd5-aa56-14aec2084d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 565s 3us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_val, y_val) = tf.keras.datasets.cifar100.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSI9pljn0gh-"
      },
      "source": [
        " Преобразуем метки классов в one_hot формат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HlhmZsL60mZS"
      },
      "outputs": [],
      "source": [
        "y_train = (y_train == np.arange(CLASSES)).astype(np.float32)\n",
        "y_val = (y_val == np.arange(CLASSES)).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xzo7yyg27jD"
      },
      "source": [
        "Убедимся, что ячейка выполняется без ошибок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pTW2isdk3Al1"
      },
      "outputs": [],
      "source": [
        "assert X_train.shape == (50000, 32, 32, 3)\n",
        "assert X_val.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 100)\n",
        "assert y_val.shape == (10000, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAdbTZ5F3SAv"
      },
      "source": [
        "Зададим архитектуру модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "isSzkfK63Ren"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(8, (2, 2), activation='selu',\n",
        "                  input_shape=(32, 32, 3), padding='same', strides = 2),\n",
        "    tf.keras.layers.Dense(100, activation='selu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.SeparableConv2D(4, (2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(100, activation='selu'),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(50, activation='selu'),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(CLASSES, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ki1NztJ8Z2y"
      },
      "source": [
        "Вычислим количество операций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uWa6vEC8YtI",
        "outputId": "10f7bdd9-47df-4b4c-eb1a-c79168bcf2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 0.6533e6\n"
          ]
        }
      ],
      "source": [
        "flops = get_flops(model, batch_size=1)\n",
        "print(f\"FLOPs: {(flops / 1e6):.4f}e6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S3RNA8FFgRZ"
      },
      "source": [
        "Информация о модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O32y6tj5Fh7k",
        "outputId": "da1987b4-d595-4a86-8504-51e22e3ad530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_39 (Conv2D)          (None, 16, 16, 8)         104       \n",
            "                                                                 \n",
            " dense_121 (Dense)           (None, 16, 16, 100)       900       \n",
            "                                                                 \n",
            " max_pooling2d_36 (MaxPoolin  (None, 8, 8, 100)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_83 (Dropout)        (None, 8, 8, 100)         0         \n",
            "                                                                 \n",
            " separable_conv2d_27 (Separa  (None, 7, 7, 4)          804       \n",
            " bleConv2D)                                                      \n",
            "                                                                 \n",
            " flatten_37 (Flatten)        (None, 196)               0         \n",
            "                                                                 \n",
            " batch_normalization_101 (Ba  (None, 196)              784       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_122 (Dense)           (None, 100)               19700     \n",
            "                                                                 \n",
            " dropout_84 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " batch_normalization_102 (Ba  (None, 100)              400       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_123 (Dense)           (None, 50)                5050      \n",
            "                                                                 \n",
            " dropout_85 (Dropout)        (None, 50)                0         \n",
            "                                                                 \n",
            " batch_normalization_103 (Ba  (None, 50)               200       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_124 (Dense)           (None, 100)               5100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,042\n",
            "Trainable params: 32,350\n",
            "Non-trainable params: 692\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2YyNZmRFyIy"
      },
      "source": [
        "Ячейка для изменения(получения более высокой точности)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "MZ_CChLgF91A"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(\n",
        "                learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=LEARNING_RATE, decay_steps=1000, decay_rate=0.5)\n",
        "            ),\n",
        "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "            metrics=['accuracy']\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1b3tvRFGKNk"
      },
      "source": [
        "Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX2VCHZgGLb7",
        "outputId": "1eff2a30-df84-40bc-efae-bf2bb140b240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256\n",
            "391/391 [==============================] - 21s 52ms/step - loss: 3.7886 - accuracy: 0.1157 - val_loss: 3.5610 - val_accuracy: 0.1644\n",
            "Epoch 2/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 3.3018 - accuracy: 0.1969 - val_loss: 3.2479 - val_accuracy: 0.2141\n",
            "Epoch 3/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 3.1485 - accuracy: 0.2253 - val_loss: 3.0878 - val_accuracy: 0.2414\n",
            "Epoch 4/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 3.0617 - accuracy: 0.2432 - val_loss: 2.9761 - val_accuracy: 0.2660\n",
            "Epoch 5/256\n",
            "391/391 [==============================] - 22s 55ms/step - loss: 2.9905 - accuracy: 0.2550 - val_loss: 2.9239 - val_accuracy: 0.2779\n",
            "Epoch 6/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.9511 - accuracy: 0.2620 - val_loss: 2.8963 - val_accuracy: 0.2878\n",
            "Epoch 7/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.9080 - accuracy: 0.2714 - val_loss: 2.8697 - val_accuracy: 0.2901\n",
            "Epoch 8/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.8818 - accuracy: 0.2773 - val_loss: 2.8690 - val_accuracy: 0.2904\n",
            "Epoch 9/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.8618 - accuracy: 0.2814 - val_loss: 2.8354 - val_accuracy: 0.2993\n",
            "Epoch 10/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.8406 - accuracy: 0.2856 - val_loss: 2.8286 - val_accuracy: 0.3018\n",
            "Epoch 11/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.8324 - accuracy: 0.2894 - val_loss: 2.8173 - val_accuracy: 0.3050\n",
            "Epoch 12/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.8237 - accuracy: 0.2898 - val_loss: 2.8154 - val_accuracy: 0.3062\n",
            "Epoch 13/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.8091 - accuracy: 0.2931 - val_loss: 2.8097 - val_accuracy: 0.3065\n",
            "Epoch 14/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.8081 - accuracy: 0.2928 - val_loss: 2.8072 - val_accuracy: 0.3057\n",
            "Epoch 15/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7969 - accuracy: 0.2936 - val_loss: 2.8076 - val_accuracy: 0.3053\n",
            "Epoch 16/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7977 - accuracy: 0.2918 - val_loss: 2.8046 - val_accuracy: 0.3091\n",
            "Epoch 17/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7973 - accuracy: 0.2960 - val_loss: 2.8038 - val_accuracy: 0.3087\n",
            "Epoch 18/256\n",
            "391/391 [==============================] - 22s 55ms/step - loss: 2.7953 - accuracy: 0.2942 - val_loss: 2.8018 - val_accuracy: 0.3079\n",
            "Epoch 19/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7879 - accuracy: 0.2955 - val_loss: 2.8022 - val_accuracy: 0.3080\n",
            "Epoch 20/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7908 - accuracy: 0.2944 - val_loss: 2.8017 - val_accuracy: 0.3096\n",
            "Epoch 21/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7901 - accuracy: 0.2943 - val_loss: 2.8007 - val_accuracy: 0.3100\n",
            "Epoch 22/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7905 - accuracy: 0.2969 - val_loss: 2.8009 - val_accuracy: 0.3102\n",
            "Epoch 23/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7884 - accuracy: 0.2950 - val_loss: 2.8012 - val_accuracy: 0.3091\n",
            "Epoch 24/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7889 - accuracy: 0.2963 - val_loss: 2.8010 - val_accuracy: 0.3101\n",
            "Epoch 25/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7892 - accuracy: 0.2948 - val_loss: 2.8005 - val_accuracy: 0.3092\n",
            "Epoch 26/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7890 - accuracy: 0.2970 - val_loss: 2.8008 - val_accuracy: 0.3095\n",
            "Epoch 27/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7948 - accuracy: 0.2949 - val_loss: 2.8005 - val_accuracy: 0.3092\n",
            "Epoch 28/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7876 - accuracy: 0.2957 - val_loss: 2.8004 - val_accuracy: 0.3096\n",
            "Epoch 29/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7874 - accuracy: 0.2959 - val_loss: 2.8001 - val_accuracy: 0.3093\n",
            "Epoch 30/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7879 - accuracy: 0.2945 - val_loss: 2.8002 - val_accuracy: 0.3098\n",
            "Epoch 31/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7868 - accuracy: 0.2961 - val_loss: 2.8003 - val_accuracy: 0.3098\n",
            "Epoch 32/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7838 - accuracy: 0.2955 - val_loss: 2.8005 - val_accuracy: 0.3101\n",
            "Epoch 33/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7838 - accuracy: 0.2952 - val_loss: 2.8006 - val_accuracy: 0.3092\n",
            "Epoch 34/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7807 - accuracy: 0.2965 - val_loss: 2.8003 - val_accuracy: 0.3086\n",
            "Epoch 35/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7858 - accuracy: 0.2967 - val_loss: 2.8002 - val_accuracy: 0.3100\n",
            "Epoch 36/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7922 - accuracy: 0.2948 - val_loss: 2.8002 - val_accuracy: 0.3089\n",
            "Epoch 37/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7884 - accuracy: 0.2958 - val_loss: 2.8005 - val_accuracy: 0.3100\n",
            "Epoch 38/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7829 - accuracy: 0.2971 - val_loss: 2.8002 - val_accuracy: 0.3095\n",
            "Epoch 39/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7887 - accuracy: 0.2937 - val_loss: 2.8003 - val_accuracy: 0.3092\n",
            "Epoch 40/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7843 - accuracy: 0.2959 - val_loss: 2.8006 - val_accuracy: 0.3093\n",
            "Epoch 41/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7824 - accuracy: 0.2949 - val_loss: 2.8007 - val_accuracy: 0.3094\n",
            "Epoch 42/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7921 - accuracy: 0.2966 - val_loss: 2.8005 - val_accuracy: 0.3096\n",
            "Epoch 43/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7892 - accuracy: 0.2948 - val_loss: 2.8005 - val_accuracy: 0.3094\n",
            "Epoch 44/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7920 - accuracy: 0.2946 - val_loss: 2.8002 - val_accuracy: 0.3097\n",
            "Epoch 45/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7909 - accuracy: 0.2957 - val_loss: 2.8004 - val_accuracy: 0.3096\n",
            "Epoch 46/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7920 - accuracy: 0.2961 - val_loss: 2.8004 - val_accuracy: 0.3096\n",
            "Epoch 47/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7878 - accuracy: 0.2973 - val_loss: 2.8003 - val_accuracy: 0.3094\n",
            "Epoch 48/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7882 - accuracy: 0.2974 - val_loss: 2.8004 - val_accuracy: 0.3088\n",
            "Epoch 49/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7869 - accuracy: 0.2958 - val_loss: 2.8003 - val_accuracy: 0.3101\n",
            "Epoch 50/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7878 - accuracy: 0.2963 - val_loss: 2.8007 - val_accuracy: 0.3089\n",
            "Epoch 51/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7896 - accuracy: 0.2962 - val_loss: 2.8002 - val_accuracy: 0.3091\n",
            "Epoch 52/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7855 - accuracy: 0.2973 - val_loss: 2.8002 - val_accuracy: 0.3088\n",
            "Epoch 53/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7848 - accuracy: 0.2962 - val_loss: 2.8004 - val_accuracy: 0.3088\n",
            "Epoch 54/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7842 - accuracy: 0.2959 - val_loss: 2.8002 - val_accuracy: 0.3087\n",
            "Epoch 55/256\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 2.7873 - accuracy: 0.2968 - val_loss: 2.8002 - val_accuracy: 0.3098\n",
            "Epoch 56/256\n",
            "391/391 [==============================] - 21s 52ms/step - loss: 2.7834 - accuracy: 0.2954 - val_loss: 2.8005 - val_accuracy: 0.3099\n",
            "Epoch 57/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7885 - accuracy: 0.2952 - val_loss: 2.8004 - val_accuracy: 0.3095\n",
            "Epoch 58/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7875 - accuracy: 0.2953 - val_loss: 2.8003 - val_accuracy: 0.3093\n",
            "Epoch 59/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7889 - accuracy: 0.2946 - val_loss: 2.8003 - val_accuracy: 0.3097\n",
            "Epoch 60/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7933 - accuracy: 0.2934 - val_loss: 2.8003 - val_accuracy: 0.3091\n",
            "Epoch 61/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7868 - accuracy: 0.2964 - val_loss: 2.8001 - val_accuracy: 0.3086\n",
            "Epoch 62/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7825 - accuracy: 0.2970 - val_loss: 2.8003 - val_accuracy: 0.3094\n",
            "Epoch 63/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7925 - accuracy: 0.2948 - val_loss: 2.8003 - val_accuracy: 0.3101\n",
            "Epoch 64/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7872 - accuracy: 0.2956 - val_loss: 2.8005 - val_accuracy: 0.3102\n",
            "Epoch 65/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7865 - accuracy: 0.2961 - val_loss: 2.8005 - val_accuracy: 0.3093\n",
            "Epoch 66/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7891 - accuracy: 0.2942 - val_loss: 2.8001 - val_accuracy: 0.3102\n",
            "Epoch 67/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7831 - accuracy: 0.2977 - val_loss: 2.8002 - val_accuracy: 0.3086\n",
            "Epoch 68/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7893 - accuracy: 0.2948 - val_loss: 2.8004 - val_accuracy: 0.3102\n",
            "Epoch 69/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7858 - accuracy: 0.2957 - val_loss: 2.8006 - val_accuracy: 0.3092\n",
            "Epoch 70/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7863 - accuracy: 0.2956 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 71/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7895 - accuracy: 0.2981 - val_loss: 2.8003 - val_accuracy: 0.3097\n",
            "Epoch 72/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7902 - accuracy: 0.2976 - val_loss: 2.8004 - val_accuracy: 0.3093\n",
            "Epoch 73/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7908 - accuracy: 0.2947 - val_loss: 2.8003 - val_accuracy: 0.3094\n",
            "Epoch 74/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7898 - accuracy: 0.2963 - val_loss: 2.8005 - val_accuracy: 0.3091\n",
            "Epoch 75/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7821 - accuracy: 0.2964 - val_loss: 2.8008 - val_accuracy: 0.3089\n",
            "Epoch 76/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7922 - accuracy: 0.2926 - val_loss: 2.8004 - val_accuracy: 0.3101\n",
            "Epoch 77/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7931 - accuracy: 0.2964 - val_loss: 2.8006 - val_accuracy: 0.3095\n",
            "Epoch 78/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7869 - accuracy: 0.2979 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 79/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7801 - accuracy: 0.2956 - val_loss: 2.8003 - val_accuracy: 0.3092\n",
            "Epoch 80/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7905 - accuracy: 0.2952 - val_loss: 2.8003 - val_accuracy: 0.3098\n",
            "Epoch 81/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7877 - accuracy: 0.2958 - val_loss: 2.8005 - val_accuracy: 0.3101\n",
            "Epoch 82/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7867 - accuracy: 0.2984 - val_loss: 2.8004 - val_accuracy: 0.3097\n",
            "Epoch 83/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7820 - accuracy: 0.2977 - val_loss: 2.8003 - val_accuracy: 0.3100\n",
            "Epoch 84/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7893 - accuracy: 0.2952 - val_loss: 2.8004 - val_accuracy: 0.3093\n",
            "Epoch 85/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7882 - accuracy: 0.2971 - val_loss: 2.8005 - val_accuracy: 0.3099\n",
            "Epoch 86/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7939 - accuracy: 0.2940 - val_loss: 2.8006 - val_accuracy: 0.3092\n",
            "Epoch 87/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7871 - accuracy: 0.2968 - val_loss: 2.8006 - val_accuracy: 0.3094\n",
            "Epoch 88/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7880 - accuracy: 0.2975 - val_loss: 2.8005 - val_accuracy: 0.3097\n",
            "Epoch 89/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7877 - accuracy: 0.2991 - val_loss: 2.8003 - val_accuracy: 0.3092\n",
            "Epoch 90/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7852 - accuracy: 0.2944 - val_loss: 2.8004 - val_accuracy: 0.3099\n",
            "Epoch 91/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7856 - accuracy: 0.2973 - val_loss: 2.8003 - val_accuracy: 0.3106\n",
            "Epoch 92/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7891 - accuracy: 0.2964 - val_loss: 2.8003 - val_accuracy: 0.3099\n",
            "Epoch 93/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7910 - accuracy: 0.2962 - val_loss: 2.8003 - val_accuracy: 0.3090\n",
            "Epoch 94/256\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 2.7864 - accuracy: 0.2967 - val_loss: 2.8004 - val_accuracy: 0.3097\n",
            "Epoch 95/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7895 - accuracy: 0.2953 - val_loss: 2.8002 - val_accuracy: 0.3098\n",
            "Epoch 96/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7835 - accuracy: 0.2960 - val_loss: 2.8005 - val_accuracy: 0.3098\n",
            "Epoch 97/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7853 - accuracy: 0.2967 - val_loss: 2.8003 - val_accuracy: 0.3098\n",
            "Epoch 98/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7891 - accuracy: 0.2961 - val_loss: 2.7999 - val_accuracy: 0.3086\n",
            "Epoch 99/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7871 - accuracy: 0.2957 - val_loss: 2.8004 - val_accuracy: 0.3093\n",
            "Epoch 100/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7916 - accuracy: 0.2957 - val_loss: 2.8003 - val_accuracy: 0.3102\n",
            "Epoch 101/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7833 - accuracy: 0.2974 - val_loss: 2.8001 - val_accuracy: 0.3095\n",
            "Epoch 102/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7853 - accuracy: 0.2959 - val_loss: 2.8003 - val_accuracy: 0.3086\n",
            "Epoch 103/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7922 - accuracy: 0.2935 - val_loss: 2.8007 - val_accuracy: 0.3096\n",
            "Epoch 104/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7886 - accuracy: 0.2958 - val_loss: 2.8005 - val_accuracy: 0.3094\n",
            "Epoch 105/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7832 - accuracy: 0.2977 - val_loss: 2.8004 - val_accuracy: 0.3089\n",
            "Epoch 106/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7888 - accuracy: 0.2963 - val_loss: 2.8004 - val_accuracy: 0.3097\n",
            "Epoch 107/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7883 - accuracy: 0.2979 - val_loss: 2.8007 - val_accuracy: 0.3075\n",
            "Epoch 108/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7880 - accuracy: 0.2992 - val_loss: 2.8004 - val_accuracy: 0.3101\n",
            "Epoch 109/256\n",
            "391/391 [==============================] - 21s 52ms/step - loss: 2.7824 - accuracy: 0.2981 - val_loss: 2.8006 - val_accuracy: 0.3092\n",
            "Epoch 110/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7870 - accuracy: 0.2969 - val_loss: 2.8002 - val_accuracy: 0.3093\n",
            "Epoch 111/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7947 - accuracy: 0.2944 - val_loss: 2.8005 - val_accuracy: 0.3089\n",
            "Epoch 112/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7867 - accuracy: 0.2946 - val_loss: 2.8006 - val_accuracy: 0.3091\n",
            "Epoch 113/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7837 - accuracy: 0.2964 - val_loss: 2.8005 - val_accuracy: 0.3091\n",
            "Epoch 114/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7848 - accuracy: 0.2962 - val_loss: 2.8005 - val_accuracy: 0.3089\n",
            "Epoch 115/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7874 - accuracy: 0.2962 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 116/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7894 - accuracy: 0.2958 - val_loss: 2.8003 - val_accuracy: 0.3093\n",
            "Epoch 117/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7852 - accuracy: 0.2973 - val_loss: 2.8004 - val_accuracy: 0.3098\n",
            "Epoch 118/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7874 - accuracy: 0.2957 - val_loss: 2.8001 - val_accuracy: 0.3096\n",
            "Epoch 119/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7851 - accuracy: 0.2964 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 120/256\n",
            "391/391 [==============================] - 21s 52ms/step - loss: 2.7904 - accuracy: 0.2956 - val_loss: 2.8001 - val_accuracy: 0.3100\n",
            "Epoch 121/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7896 - accuracy: 0.2956 - val_loss: 2.8001 - val_accuracy: 0.3084\n",
            "Epoch 122/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7870 - accuracy: 0.2966 - val_loss: 2.8004 - val_accuracy: 0.3095\n",
            "Epoch 123/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7877 - accuracy: 0.2966 - val_loss: 2.8002 - val_accuracy: 0.3097\n",
            "Epoch 124/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7833 - accuracy: 0.2965 - val_loss: 2.8007 - val_accuracy: 0.3100\n",
            "Epoch 125/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7838 - accuracy: 0.2971 - val_loss: 2.8004 - val_accuracy: 0.3100\n",
            "Epoch 126/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7873 - accuracy: 0.2965 - val_loss: 2.8007 - val_accuracy: 0.3085\n",
            "Epoch 127/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7844 - accuracy: 0.2985 - val_loss: 2.8004 - val_accuracy: 0.3097\n",
            "Epoch 128/256\n",
            "391/391 [==============================] - 23s 58ms/step - loss: 2.7908 - accuracy: 0.2949 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 129/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7881 - accuracy: 0.2957 - val_loss: 2.8005 - val_accuracy: 0.3095\n",
            "Epoch 130/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7886 - accuracy: 0.2926 - val_loss: 2.8005 - val_accuracy: 0.3099\n",
            "Epoch 131/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7860 - accuracy: 0.2972 - val_loss: 2.8005 - val_accuracy: 0.3093\n",
            "Epoch 132/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7898 - accuracy: 0.2934 - val_loss: 2.8005 - val_accuracy: 0.3097\n",
            "Epoch 133/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7862 - accuracy: 0.2966 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 134/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7897 - accuracy: 0.2948 - val_loss: 2.8006 - val_accuracy: 0.3096\n",
            "Epoch 135/256\n",
            "391/391 [==============================] - 21s 52ms/step - loss: 2.7840 - accuracy: 0.2959 - val_loss: 2.8008 - val_accuracy: 0.3093\n",
            "Epoch 136/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7866 - accuracy: 0.2956 - val_loss: 2.8004 - val_accuracy: 0.3094\n",
            "Epoch 137/256\n",
            "391/391 [==============================] - 21s 52ms/step - loss: 2.7874 - accuracy: 0.2946 - val_loss: 2.8004 - val_accuracy: 0.3091\n",
            "Epoch 138/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7916 - accuracy: 0.2964 - val_loss: 2.8003 - val_accuracy: 0.3090\n",
            "Epoch 139/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7848 - accuracy: 0.2959 - val_loss: 2.8005 - val_accuracy: 0.3105\n",
            "Epoch 140/256\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 2.7850 - accuracy: 0.2986 - val_loss: 2.8005 - val_accuracy: 0.3100\n",
            "Epoch 141/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7859 - accuracy: 0.2968 - val_loss: 2.8006 - val_accuracy: 0.3099\n",
            "Epoch 142/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7846 - accuracy: 0.2975 - val_loss: 2.8004 - val_accuracy: 0.3086\n",
            "Epoch 143/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7851 - accuracy: 0.2951 - val_loss: 2.8003 - val_accuracy: 0.3102\n",
            "Epoch 144/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7830 - accuracy: 0.2978 - val_loss: 2.8004 - val_accuracy: 0.3093\n",
            "Epoch 145/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7884 - accuracy: 0.2955 - val_loss: 2.8004 - val_accuracy: 0.3094\n",
            "Epoch 146/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7903 - accuracy: 0.2945 - val_loss: 2.8005 - val_accuracy: 0.3095\n",
            "Epoch 147/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7893 - accuracy: 0.2970 - val_loss: 2.8007 - val_accuracy: 0.3101\n",
            "Epoch 148/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7953 - accuracy: 0.2935 - val_loss: 2.8002 - val_accuracy: 0.3091\n",
            "Epoch 149/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7839 - accuracy: 0.2963 - val_loss: 2.8001 - val_accuracy: 0.3097\n",
            "Epoch 150/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7862 - accuracy: 0.2987 - val_loss: 2.8008 - val_accuracy: 0.3095\n",
            "Epoch 151/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7839 - accuracy: 0.2983 - val_loss: 2.8006 - val_accuracy: 0.3094\n",
            "Epoch 152/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7870 - accuracy: 0.2952 - val_loss: 2.8004 - val_accuracy: 0.3092\n",
            "Epoch 153/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7885 - accuracy: 0.2961 - val_loss: 2.8004 - val_accuracy: 0.3097\n",
            "Epoch 154/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7854 - accuracy: 0.2968 - val_loss: 2.8006 - val_accuracy: 0.3103\n",
            "Epoch 155/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7838 - accuracy: 0.2979 - val_loss: 2.8003 - val_accuracy: 0.3103\n",
            "Epoch 156/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7853 - accuracy: 0.2965 - val_loss: 2.8008 - val_accuracy: 0.3103\n",
            "Epoch 157/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7856 - accuracy: 0.2964 - val_loss: 2.8003 - val_accuracy: 0.3091\n",
            "Epoch 158/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7821 - accuracy: 0.2960 - val_loss: 2.8007 - val_accuracy: 0.3094\n",
            "Epoch 159/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7893 - accuracy: 0.2965 - val_loss: 2.8004 - val_accuracy: 0.3099\n",
            "Epoch 160/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7899 - accuracy: 0.2931 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 161/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7923 - accuracy: 0.2968 - val_loss: 2.8003 - val_accuracy: 0.3095\n",
            "Epoch 162/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7841 - accuracy: 0.2957 - val_loss: 2.8010 - val_accuracy: 0.3089\n",
            "Epoch 163/256\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 2.7844 - accuracy: 0.2966 - val_loss: 2.8005 - val_accuracy: 0.3087\n",
            "Epoch 164/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7881 - accuracy: 0.2952 - val_loss: 2.8005 - val_accuracy: 0.3090\n",
            "Epoch 165/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7886 - accuracy: 0.2970 - val_loss: 2.8005 - val_accuracy: 0.3094\n",
            "Epoch 166/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7842 - accuracy: 0.2957 - val_loss: 2.8004 - val_accuracy: 0.3091\n",
            "Epoch 167/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7867 - accuracy: 0.2956 - val_loss: 2.8005 - val_accuracy: 0.3093\n",
            "Epoch 168/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7916 - accuracy: 0.2955 - val_loss: 2.8001 - val_accuracy: 0.3096\n",
            "Epoch 169/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7871 - accuracy: 0.2951 - val_loss: 2.8002 - val_accuracy: 0.3090\n",
            "Epoch 170/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7879 - accuracy: 0.2976 - val_loss: 2.8006 - val_accuracy: 0.3087\n",
            "Epoch 171/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7869 - accuracy: 0.2971 - val_loss: 2.8002 - val_accuracy: 0.3094\n",
            "Epoch 172/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7904 - accuracy: 0.2972 - val_loss: 2.8006 - val_accuracy: 0.3099\n",
            "Epoch 173/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7861 - accuracy: 0.2963 - val_loss: 2.8007 - val_accuracy: 0.3084\n",
            "Epoch 174/256\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 2.7907 - accuracy: 0.2974 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 175/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7876 - accuracy: 0.2973 - val_loss: 2.8005 - val_accuracy: 0.3098\n",
            "Epoch 176/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7848 - accuracy: 0.2970 - val_loss: 2.8006 - val_accuracy: 0.3093\n",
            "Epoch 177/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7896 - accuracy: 0.2955 - val_loss: 2.8001 - val_accuracy: 0.3092\n",
            "Epoch 178/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7909 - accuracy: 0.2960 - val_loss: 2.8003 - val_accuracy: 0.3093\n",
            "Epoch 179/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7939 - accuracy: 0.2959 - val_loss: 2.8004 - val_accuracy: 0.3087\n",
            "Epoch 180/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7877 - accuracy: 0.2952 - val_loss: 2.8002 - val_accuracy: 0.3094\n",
            "Epoch 181/256\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.7880 - accuracy: 0.2946 - val_loss: 2.8002 - val_accuracy: 0.3097\n",
            "Epoch 182/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7831 - accuracy: 0.2953 - val_loss: 2.8003 - val_accuracy: 0.3091\n",
            "Epoch 183/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7886 - accuracy: 0.2954 - val_loss: 2.8002 - val_accuracy: 0.3092\n",
            "Epoch 184/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7890 - accuracy: 0.2961 - val_loss: 2.8002 - val_accuracy: 0.3097\n",
            "Epoch 185/256\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 2.7945 - accuracy: 0.2960 - val_loss: 2.8004 - val_accuracy: 0.3087\n",
            "Epoch 186/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7920 - accuracy: 0.2923 - val_loss: 2.8000 - val_accuracy: 0.3090\n",
            "Epoch 187/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7945 - accuracy: 0.2947 - val_loss: 2.8002 - val_accuracy: 0.3097\n",
            "Epoch 188/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7910 - accuracy: 0.2957 - val_loss: 2.8003 - val_accuracy: 0.3092\n",
            "Epoch 189/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7889 - accuracy: 0.2955 - val_loss: 2.8004 - val_accuracy: 0.3089\n",
            "Epoch 190/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7832 - accuracy: 0.2974 - val_loss: 2.8003 - val_accuracy: 0.3092\n",
            "Epoch 191/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7854 - accuracy: 0.2966 - val_loss: 2.8005 - val_accuracy: 0.3086\n",
            "Epoch 192/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7850 - accuracy: 0.2988 - val_loss: 2.8002 - val_accuracy: 0.3095\n",
            "Epoch 193/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7856 - accuracy: 0.2954 - val_loss: 2.8003 - val_accuracy: 0.3098\n",
            "Epoch 194/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7874 - accuracy: 0.2988 - val_loss: 2.8006 - val_accuracy: 0.3101\n",
            "Epoch 195/256\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 2.7881 - accuracy: 0.2964 - val_loss: 2.8001 - val_accuracy: 0.3093\n",
            "Epoch 196/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7839 - accuracy: 0.2959 - val_loss: 2.8004 - val_accuracy: 0.3096\n",
            "Epoch 197/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7898 - accuracy: 0.2960 - val_loss: 2.8005 - val_accuracy: 0.3096\n",
            "Epoch 198/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7933 - accuracy: 0.2939 - val_loss: 2.8005 - val_accuracy: 0.3090\n",
            "Epoch 199/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7893 - accuracy: 0.2970 - val_loss: 2.8004 - val_accuracy: 0.3105\n",
            "Epoch 200/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7883 - accuracy: 0.2969 - val_loss: 2.8003 - val_accuracy: 0.3090\n",
            "Epoch 201/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7868 - accuracy: 0.2980 - val_loss: 2.8002 - val_accuracy: 0.3096\n",
            "Epoch 202/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7905 - accuracy: 0.2946 - val_loss: 2.8001 - val_accuracy: 0.3096\n",
            "Epoch 203/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7851 - accuracy: 0.2969 - val_loss: 2.8004 - val_accuracy: 0.3094\n",
            "Epoch 204/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7928 - accuracy: 0.2957 - val_loss: 2.8003 - val_accuracy: 0.3093\n",
            "Epoch 205/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7925 - accuracy: 0.2963 - val_loss: 2.8002 - val_accuracy: 0.3090\n",
            "Epoch 206/256\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 2.7926 - accuracy: 0.2933 - val_loss: 2.8005 - val_accuracy: 0.3101\n",
            "Epoch 207/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7900 - accuracy: 0.2969 - val_loss: 2.8004 - val_accuracy: 0.3087\n",
            "Epoch 208/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7869 - accuracy: 0.2940 - val_loss: 2.8003 - val_accuracy: 0.3086\n",
            "Epoch 209/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7829 - accuracy: 0.2953 - val_loss: 2.8003 - val_accuracy: 0.3087\n",
            "Epoch 210/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7885 - accuracy: 0.2962 - val_loss: 2.8003 - val_accuracy: 0.3098\n",
            "Epoch 211/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7901 - accuracy: 0.2963 - val_loss: 2.8004 - val_accuracy: 0.3097\n",
            "Epoch 212/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7828 - accuracy: 0.2953 - val_loss: 2.8003 - val_accuracy: 0.3094\n",
            "Epoch 213/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7909 - accuracy: 0.2973 - val_loss: 2.8008 - val_accuracy: 0.3088\n",
            "Epoch 214/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7872 - accuracy: 0.2971 - val_loss: 2.8004 - val_accuracy: 0.3090\n",
            "Epoch 215/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7859 - accuracy: 0.2958 - val_loss: 2.8007 - val_accuracy: 0.3083\n",
            "Epoch 216/256\n",
            "391/391 [==============================] - 23s 58ms/step - loss: 2.7848 - accuracy: 0.2965 - val_loss: 2.8008 - val_accuracy: 0.3095\n",
            "Epoch 217/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7867 - accuracy: 0.2954 - val_loss: 2.8007 - val_accuracy: 0.3098\n",
            "Epoch 218/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7879 - accuracy: 0.2946 - val_loss: 2.8003 - val_accuracy: 0.3093\n",
            "Epoch 219/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7930 - accuracy: 0.2940 - val_loss: 2.8002 - val_accuracy: 0.3087\n",
            "Epoch 220/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7882 - accuracy: 0.2951 - val_loss: 2.8004 - val_accuracy: 0.3094\n",
            "Epoch 221/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7860 - accuracy: 0.2957 - val_loss: 2.8005 - val_accuracy: 0.3098\n",
            "Epoch 222/256\n",
            "391/391 [==============================] - 21s 53ms/step - loss: 2.7889 - accuracy: 0.2954 - val_loss: 2.8006 - val_accuracy: 0.3095\n",
            "Epoch 223/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7797 - accuracy: 0.2980 - val_loss: 2.8005 - val_accuracy: 0.3089\n",
            "Epoch 224/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7875 - accuracy: 0.2964 - val_loss: 2.8005 - val_accuracy: 0.3091\n",
            "Epoch 225/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7889 - accuracy: 0.2965 - val_loss: 2.8004 - val_accuracy: 0.3093\n",
            "Epoch 226/256\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 2.7835 - accuracy: 0.2958 - val_loss: 2.8007 - val_accuracy: 0.3097\n",
            "Epoch 227/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7898 - accuracy: 0.2943 - val_loss: 2.8003 - val_accuracy: 0.3097\n",
            "Epoch 228/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7880 - accuracy: 0.2965 - val_loss: 2.8005 - val_accuracy: 0.3091\n",
            "Epoch 229/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7857 - accuracy: 0.2953 - val_loss: 2.8005 - val_accuracy: 0.3092\n",
            "Epoch 230/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7834 - accuracy: 0.2967 - val_loss: 2.8003 - val_accuracy: 0.3100\n",
            "Epoch 231/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7910 - accuracy: 0.2969 - val_loss: 2.8002 - val_accuracy: 0.3090\n",
            "Epoch 232/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7866 - accuracy: 0.2976 - val_loss: 2.8006 - val_accuracy: 0.3099\n",
            "Epoch 233/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7930 - accuracy: 0.2944 - val_loss: 2.8003 - val_accuracy: 0.3089\n",
            "Epoch 234/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7932 - accuracy: 0.2958 - val_loss: 2.8002 - val_accuracy: 0.3098\n",
            "Epoch 235/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7878 - accuracy: 0.2963 - val_loss: 2.8006 - val_accuracy: 0.3097\n",
            "Epoch 236/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7888 - accuracy: 0.2974 - val_loss: 2.8003 - val_accuracy: 0.3099\n",
            "Epoch 237/256\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 2.7892 - accuracy: 0.2937 - val_loss: 2.7999 - val_accuracy: 0.3085\n",
            "Epoch 238/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7870 - accuracy: 0.2948 - val_loss: 2.8006 - val_accuracy: 0.3098\n",
            "Epoch 239/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7889 - accuracy: 0.2937 - val_loss: 2.8001 - val_accuracy: 0.3092\n",
            "Epoch 240/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7883 - accuracy: 0.2960 - val_loss: 2.8006 - val_accuracy: 0.3102\n",
            "Epoch 241/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7850 - accuracy: 0.2947 - val_loss: 2.8002 - val_accuracy: 0.3092\n",
            "Epoch 242/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7847 - accuracy: 0.2954 - val_loss: 2.8002 - val_accuracy: 0.3094\n",
            "Epoch 243/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7891 - accuracy: 0.2984 - val_loss: 2.8006 - val_accuracy: 0.3095\n",
            "Epoch 244/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7898 - accuracy: 0.2941 - val_loss: 2.8006 - val_accuracy: 0.3098\n",
            "Epoch 245/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7884 - accuracy: 0.2949 - val_loss: 2.8006 - val_accuracy: 0.3097\n",
            "Epoch 246/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7908 - accuracy: 0.2940 - val_loss: 2.8001 - val_accuracy: 0.3086\n",
            "Epoch 247/256\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 2.7884 - accuracy: 0.2942 - val_loss: 2.8002 - val_accuracy: 0.3089\n",
            "Epoch 248/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7864 - accuracy: 0.2962 - val_loss: 2.8003 - val_accuracy: 0.3095\n",
            "Epoch 249/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7885 - accuracy: 0.2946 - val_loss: 2.8003 - val_accuracy: 0.3093\n",
            "Epoch 250/256\n",
            "391/391 [==============================] - 22s 55ms/step - loss: 2.7880 - accuracy: 0.2953 - val_loss: 2.8003 - val_accuracy: 0.3099\n",
            "Epoch 251/256\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 2.7856 - accuracy: 0.2975 - val_loss: 2.8004 - val_accuracy: 0.3092\n",
            "Epoch 252/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7870 - accuracy: 0.2964 - val_loss: 2.8001 - val_accuracy: 0.3097\n",
            "Epoch 253/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7903 - accuracy: 0.2956 - val_loss: 2.8005 - val_accuracy: 0.3094\n",
            "Epoch 254/256\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 2.7901 - accuracy: 0.2961 - val_loss: 2.8001 - val_accuracy: 0.3093\n",
            "Epoch 255/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7937 - accuracy: 0.2929 - val_loss: 2.8006 - val_accuracy: 0.3092\n",
            "Epoch 256/256\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 2.7843 - accuracy: 0.2949 - val_loss: 2.8004 - val_accuracy: 0.3091\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbea2634070>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "model.fit(\n",
        "            x=X_train,\n",
        "            y=y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            callbacks=[\n",
        "                tf.keras.callbacks.ModelCheckpoint(filepath=\"{epoch:02d}-{val_accuracy:.2f}.hdf5\", save_best_only=True),           \n",
        "            ],\n",
        "            use_multiprocessing=True,\n",
        "            workers=8,\n",
        "            epochs=256\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/OOYbzfoJdzfRMstvVPhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}